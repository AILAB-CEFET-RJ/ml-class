{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppcic_ml_naivebayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9rY7f9QcRg2oi8zcoa6t0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLRG-CEFET-RJ/ml-class/blob/master/ppcic_ml_naivebayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBmQS9s945nl"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The Naive Bayes Classifier (like a logistic regression model) is a  probabilistic classification model. In order to understand that, assume that, after being trained, a NB classifier outputs the value 0.95 for a binary classification problem. This means that the classifier predicts that the new example is positive with probability 0.95, and that this example is negative with probability 0.05.\n",
        "\n",
        "The model generated by the Naive Bayes algorithm is a set of *conditional probabilities*. See a nice explanation about conditional probabilities [here](https://setosa.io/conditional/). These probabilities are estimated from the training data, as we shall see bellow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0R25fD2rFT"
      },
      "source": [
        "# Bayes' theorem\n",
        "\n",
        "The term \"bayesian\" comes from Thomas Bayes, the name of a British Presbyterian minister who lived in the 18th century.\n",
        "\n",
        "![link text](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Thomas_Bayes.gif/225px-Thomas_Bayes.gif)\n",
        "\n",
        "Thomas bayes formulated the famous [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). This theorem is the basis for the Naive Bayes Classifier, and is defined as the following:\n",
        "\n",
        "$$\n",
        "\\Pr(y \\mid x_1, x_2, \\dots, x_n) = \\frac{\\Pr (y) \\Pr (x_1, x_2, \\dots x_n \\mid y)} {\\Pr(x_1, x_2, \\dots, x_n)}\n",
        "$$\n",
        "\n",
        "The following is a description of each term in the above expression, in the context of a classification task.\n",
        "\n",
        "* $\\Pr(y \\mid x_1, \\dots, x_n)$ represents the probability of the class $y$, given the values ​​of the attributes of the example $\\mathbf{x}$. This term, called **posterior probability**, is what must be determined (learned) by the algorithm.\n",
        "\n",
        "* $\\Pr(x_1, \\dots x_n \\mid y)$ represents the probability that a specific combination of values ​​$x_1, \\dots, x_n$ will occur in examples associated with a specific value of the target attribute $y$. This term is called **likelihood**.\n",
        "\n",
        "* $\\Pr(y)$ represents the probability that an example selected at random belongs to a given class (i.e., belongs to a given value of the target attribute $y$). This term is called **prior probability**\n",
        "\n",
        "* $\\Pr (x_1, x_2, \\dots, x_n) $ represents the probability that a given combination of values ​​$ x_1, x_2, \\dots, x_n$ will occur in an example selected at random.\n",
        "\n",
        "These probabilities are actually estimated from the training dataset by the Naive Bayes algorithm. These estimates are computed by counting the occurrences of values in a given feature, either separately  or in conjunction with values of other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZPpo1nB-8-Y"
      },
      "source": [
        "# Estimating probabilities from data\n",
        "\n",
        "Let us see an example of how probability estimates can be computed in a dataset. For this, consider the [Play Tenis dataset](https://www.kaggle.com/fredericobreno/play-tennis), which is another toy dataset with four predictors (`outlook`, `temp`, `humidity`, and `wind`) and fourteen examples. The target (`play`) is binary. Each example provides data about the weather condition in a particular day. Therefore, the classification task if to predict whether a given day is appropriate to play tenis or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I5IkDcb68a6",
        "outputId": "fa33f0f6-28f0-460d-a96e-3c8d97b53b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "import pandas as pd\n",
        "df_play_tennis = pd.read_csv('https://raw.githubusercontent.com/MLRG-CEFET-RJ/ml-class/master/datasets/play_tennis.csv')\n",
        "df_play_tennis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day</th>\n",
              "      <th>outlook</th>\n",
              "      <th>temp</th>\n",
              "      <th>humidity</th>\n",
              "      <th>wind</th>\n",
              "      <th>play</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>D1</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D2</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>D3</td>\n",
              "      <td>Overcast</td>\n",
              "      <td>Hot</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>D4</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>D5</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>D6</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>D7</td>\n",
              "      <td>Overcast</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Strong</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>D8</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Weak</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>D9</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Cool</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>D10</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>D11</td>\n",
              "      <td>Sunny</td>\n",
              "      <td>Mild</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Strong</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>D12</td>\n",
              "      <td>Overcast</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>D13</td>\n",
              "      <td>Overcast</td>\n",
              "      <td>Hot</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Weak</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>D14</td>\n",
              "      <td>Rain</td>\n",
              "      <td>Mild</td>\n",
              "      <td>High</td>\n",
              "      <td>Strong</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    day   outlook  temp humidity    wind play\n",
              "0    D1     Sunny   Hot     High    Weak   No\n",
              "1    D2     Sunny   Hot     High  Strong   No\n",
              "2    D3  Overcast   Hot     High    Weak  Yes\n",
              "3    D4      Rain  Mild     High    Weak  Yes\n",
              "4    D5      Rain  Cool   Normal    Weak  Yes\n",
              "5    D6      Rain  Cool   Normal  Strong   No\n",
              "6    D7  Overcast  Cool   Normal  Strong  Yes\n",
              "7    D8     Sunny  Mild     High    Weak   No\n",
              "8    D9     Sunny  Cool   Normal    Weak  Yes\n",
              "9   D10      Rain  Mild   Normal    Weak  Yes\n",
              "10  D11     Sunny  Mild   Normal  Strong  Yes\n",
              "11  D12  Overcast  Mild     High  Strong  Yes\n",
              "12  D13  Overcast   Hot   Normal    Weak  Yes\n",
              "13  D14      Rain  Mild     High  Strong   No"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Au2EO4AD7q"
      },
      "source": [
        "## Estimates for prior probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckwkHH0fAtJy"
      },
      "source": [
        "Let us see some examples of probability estimates that can be computed from the above dataset. First, let us compute the estimates for the prior probabilites.\n",
        "\n",
        "* $\\Pr(\\text{play} = \\text{'Yes'}) \\approx \\frac{5}{14} \\approx 36\\%$\n",
        "\n",
        "* $\\Pr(\\text{play} = \\text{'No'}) \\approx \\frac{9}{14} \\approx 64\\%$\n",
        "\n",
        "The way to interpret these prior probabilities is the following: if you do not know anything about the weather conditions in a given day, then there is approximately 64% chance that this day is appropriate to play tenis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxbcFcqhiU3u"
      },
      "source": [
        "## Estimates for likelihoods\n",
        "\n",
        "We can also compute estimate for the so called *conditional probability*, aka likelihoods.\n",
        "\n",
        "For example, let us estimate $\\Pr(\\text{outlook} = \\text{'Sunny'}  \\mid \\text{play} = \\text{'No'})$:\n",
        "\n",
        "$$\n",
        "\\Pr(\\text{outlook} = \\text{'Sunny'}  \\mid \\text{play} = \\text{'No'}) \\approx \\frac{3}{5}.\n",
        "$$\n",
        "\n",
        "It is also easy to compute estimate for probabilities like $\\Pr(\\text{outlook} = \\text{'Sunny'} \\text{ and } \\text{temp} = \\text{'Hot'} \\mid \\text{play} = \\text{'No'})$:\n",
        "\n",
        "$$\n",
        "\\Pr(\\text{outlook} = \\text{'Sunny'} \\text{ and } \\text{temp} = \\text{'Hot'} \\mid \\text{play} = \\text{'No'}) \\approx \\frac{2}{5} = 40\\%\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZsnGcSjEy75"
      },
      "source": [
        "As an exercise, compute estimates for the following probatilities (likelihoods):\n",
        "\n",
        "- $\\Pr(\\text{outlook} = \\text{'Sunny'} \\mid \\text{play} = \\text{'Yes'}) \\approx \\text{_____}$\n",
        "- $\\Pr(\\text{outlook} = \\text{Sunny} \\mid \\text{play} = \\text{'No'}) \\approx \\text{_____}$\n",
        "\n",
        "- $\\Pr(\\text{temp} = \\text{'Hot'} \\mid \\text{play} = \\text{'Yes'}) \\approx \\text{_____}$\n",
        "- $\\Pr(\\text{temp} = \\text{'Hot'} \\mid \\text{play} = \\text{'No'}) \\approx \\text{_____}$\n",
        "\n",
        "- $\\Pr(\\text{humidity} = \\text{'High'} \\mid \\text{play} = \\text{'Yes'}) \\approx \\text{_____}$\n",
        "- $\\Pr(\\text{humidity} = \\text{'High'} \\mid \\text{play} = \\text{'No'}) \\approx \\text{_____}$\n",
        "\n",
        "- $\\Pr(\\text{wind} = \\text{'Weak'} \\mid \\text{play} = \\text{'Yes'}) \\approx \\text{_____}$\n",
        "- $\\Pr(\\text{wind} = \\text{'Weak'} \\mid \\text{play} = \\text{'No'}) \\approx \\text{_____}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W667s7SxjZdN"
      },
      "source": [
        "## Estimates for posterior probabilities\n",
        "\n",
        "As a final example of computing probability estimates, let us consider $\\Pr(\\text{play} = \\text{'No'}  \\mid \\text{outlook} = \\text{'Sunny'})$:\n",
        "\n",
        "$$\n",
        "\\Pr(\\text{play} = \\text{'No'}  \\mid \\text{outlook} = \\text{'Sunny'}) \\approx \\frac{3}{5} = 60\\%\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFFnKkPitdlR"
      },
      "source": [
        "# Independence and conditional independence\n",
        "\n",
        "Recall the estimate $\\Pr(\\text{play} = \\text{'No'}  \\mid \\text{outlook} = \\text{'Sunny'}) \\approx 60\\%$. This estimate tells us that, if you are in a sunny day, then the chance is $60$% that this is not a good day to play tennis. Now, compare this value with the estimate for $\\Pr(\\text{play} = \\text{'No'}) \\approx 36\\%$. We can conclude that, knowing that we are in a sunny day changes our bets that this day is appropriate to play tenis. In other words, it seems to exist a **dependence** between variables `play` and `outlook`. \n",
        "\n",
        "In general, two random variables (events) $A$ and $B$ are said to be independent if and only if both identities below are true:\n",
        "\n",
        "1. $\\Pr(A \\mid B) = \\Pr(A)$\n",
        "\n",
        "2. $\\Pr(B \\mid A) = \\Pr(B)$\n",
        "\n",
        "Another related concept is *conditional independence*. Given three variables A, B, and C. We say that variables A and B are conditionally independet given the variable C if and only if knowing the value of C makes A and B independent of each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssZtdQAdM119"
      },
      "source": [
        "# Steps of the Naive Bayes algorithm\n",
        "\n",
        "Naive Bayes is an algorithm consisting of two steps, which are described below. Formally, let $X$ be a dataset. Also consider that $c_1, c_2, \\ldots, c_k$ are the classes of the problem (i.e., the possible values ​​of the target feature) and that $\\mathbf{x} = [x_1, x_2, ..., x_n]$ is a new example that should be classified. Let $a_1, a_2, ..., a_n$ be the values for the predictive features $x_1, x_2, ..., x_n$, respectively.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. Calculate the posterior probabilities $\\Pr(y = c_j \\mid \\mathbf{x})$, $j = 1,2, \\ldots, k $\n",
        "2. Classify $\\mathbf{x}$ as being of class $c$ such that $\\Pr(y = c \\mid \\mathbf{x})$ is maximum.\n",
        "\n",
        "\n",
        "The term *naive* stems from the fact that Naive Bayes assumes that, given a value of the target $y$, the predictive features are statistically independent from each other. By considering this to be true, the computation of the conditional probabilities can be simplified like this:\n",
        "\n",
        "$$\n",
        "\\Pr(x_1, x_2, \\dots x_n \\mid y) = \\Pr(x_1 \\mid y) \\times \\Pr(x_2 \\mid y) \\times \\ldots \\times \\Pr(x_n \\mid y)\n",
        "$$\n",
        "\n",
        "In many practical cases, this statistical independence between predictors assumed by NB does not exist. For example, consider a dataset with information about customers of a company. Also consider that each customer is represented by the following features: *weight*, *education*, *salary*, *age*, etc. In this dataset, the values ​​of the first three feature are correlated with values ​​of the age. In this case, at least in theory, the use of Naive Bayes would overestimate the effect of the age feature. However, practice shows that Naive Bayes is quite effective even in cases where the predictive features are not statistically independent.\n",
        "\n",
        "Anyway, assuming the naive hypothesis is true, we can simplify the Bayes formula:\n",
        "\n",
        "$$\n",
        "\\Pr(y \\mid x_1, x_2, \\dots, x_n) \\propto \\Pr(y) \\times \\Pr(x_1 \\mid y) \\times \\Pr(x_2 \\mid y) \\times \\ldots \\Pr(x_n \\mid y)\n",
        "$$\n",
        "\n",
        "Therefore, to compute the probability that an example $\\mathbf{x}$ belongs to a given class, we just need the estimates for $\\Pr(y)$ and for $\\Pr(x_i \\mid y)$, which you already know how to compute (see the above examples for the Play Tenis dataset). Together, these probability values represent the model generated by the Naive Bayes algorithm. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT__Dg-YTZ_L"
      },
      "source": [
        "Consider the following question is: is it appropriate or 'No't to play tennis on a 'Sunny', 'Hot', 'High' humidity and light wind day? This question is equivalent to classifying an example $\\mathbf{x}$ corresponding to $[\\text{outlook} = \\text{'Sunny'}, \\text{temp} = \\text{'Hot'}, \\text{humidity} = \\text{'High'}, \\text{wind} = \\text{Weak}]$. To answer this question, we can apply the Naive Ba'Yes' classifier. \n",
        "\n",
        "For the propabilities $\\Pr(y)$, we find that $\\Pr(\\text{play} = \\text{'Yes'}) \\approx 9/14$ and $\\Pr(\\text{play} = \\text{'No'}) \\approx 5/14$. \n",
        "\n",
        "Similarly, estimates for conditional probabilities $\\Pr(x_i \\mid y)$ are calculated:\n",
        "\n",
        "- $\\Pr(\\text{outlook} = \\text{'Sunny'} \\mid \\text{play} = \\text{'Yes'}) \\approx 5/9$\n",
        "- $\\Pr(\\text{outlook} = \\text{'Sunny'} \\mid \\text{play} = \\text{'No'}) \\approx 2/5$\n",
        "\n",
        "- $\\Pr(\\text{temp} = \\text{'Hot'} \\mid \\text{play} = \\text{'Yes'}) \\approx 2/9$\n",
        "- $\\Pr(\\text{temp} = \\text{'Hot'} \\mid \\text{play} = \\text{'No'}) \\approx 2/5$\n",
        "\n",
        "- $\\Pr(\\text{humidity} = \\text{'High'} \\mid \\text{play} = \\text{'Yes'}) \\approx 3/9$\n",
        "- $\\Pr(\\text{humidity} = \\text{'High'} \\mid \\text{play} = \\text{'No'}) \\approx 4/5$\n",
        "\n",
        "- $\\Pr(\\text{wind} = \\text{Weak} \\mid \\text{play} = \\text{'Yes'}) \\approx 6/9$\n",
        "- $\\Pr(\\text{wind} = \\text{Weak} \\mid \\text{play} = \\text{'No'}) \\approx 2/5$\n",
        "\n",
        "The posterior probabilitis $\\Pr(\\text{play} = \\text{'Yes'} \\mid \\mathbf{x})$ and $\\Pr(\\text{play} = \\text{'No'} \\mid \\mathbf{x})$ can 'No'w be computed. In the following, we ommit the feature names, for simplicity's sake.\n",
        "\n",
        "For $\\Pr(\\text{play} = \\text{'Yes'} \\mid \\mathbf{x})$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\Pr(\\text{play} = \\text{'Yes'} \\mid \\mathbf{x}) & \\propto \\Pr(\\text{'Sunny'} \\mid \\text{'Yes'}) \\times \\Pr(\\text{Warm} \\mid \\text{'Yes'}) \\times \\Pr(\\text{'High'} \\mid \\text{'Yes'}) \\times \\Pr(\\text{Wind} \\mid \\text{'Yes'}) \\times \\Pr(\\text{'Yes'}) = \\\\\n",
        "&= 0.0071 \\times 9/14 = \\\\\n",
        "&= 0.004564286.\n",
        "\\end{align*}\n",
        "\n",
        "For $\\Pr(\\text{play} = \\text{'No'} \\mid \\mathbf{x})$:\n",
        "\n",
        "\\begin{align*}\n",
        "\\Pr(\\text{play} = \\text{'No'} \\mid \\mathbf{x}) & \\propto \\Pr(\\text{'Sunny'} \\mid \\text{'No'}) \\times \\Pr(\\text{Warm} \\mid \\text{'No'}) \\times \\Pr(\\text{'High'} \\mid \\text{'No'}) \\times \\Pr(\\text{Wind} \\mid \\text{'No'}) \\times \\Pr(\\text{'No'}) = \\\\\n",
        "&= 0.0274 \\times 5/14 = \\\\\n",
        "&= 0.009785714.\n",
        "\\end{align*}\n",
        "\n",
        "'No'w, since $\\Pr(\\text{play} = \\text{'Yes'} \\mid \\mathbf{x}) + \\Pr(\\text{play} = \\text{'No'} \\mid \\mathbf{x})$ = 1$, the numbers aborve can be mapped back to probabilities:\n",
        "\n",
        "\\begin{align*}\n",
        "\\Pr(\\text{play} &= \\text{'Yes'} \\mid \\mathbf{x}) = \\frac{0.004564286}{0.004564286+0.009785714} \\approx 32\\%\\\\\n",
        "\\\\\n",
        "\\Pr(\\text{play} &= \\text{'No'} \\mid \\mathbf{x}) = \\frac{0.009785714}{0.004564286+0.009785714} \\approx 68\\%\\end{align*}\n",
        "\n",
        "\n",
        "Since the 'High'est obtained value corresponds to $\\text{play} = \\text{'No'}$, then the class Naive Ba'Yes' predicts for $\\mathbf{x}$ is $\\text{'No'}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQePXIuZ_VJD",
        "outputId": "768620ea-e369-41eb-9c7a-8d5a61eb4495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(0.004564286/(0.004564286+0.009785714))\n",
        "print(0.009785714/(0.004564286+0.009785714))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.31806871080139376\n",
            "0.6819312891986063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBUT7J5PUB3_"
      },
      "source": [
        "# Implementation detail: using log-probabilities\n",
        "\n",
        "Recall that the posterior probability is computed by the using the following expression:\n",
        "\n",
        "$$\n",
        "\\Pr(x_1, x_2, \\dots x_n \\mid y_k) \\propto \\Pr(y_k) \\times \\Pr(x_1 \\mid y_k) \\times \\Pr(x_2 \\mid y_k) \\times \\ldots \\times \\Pr(x_n \\mid y_k)\n",
        "$$\n",
        "\n",
        "One problem when implementing the above computation is that it could result in floating point underflow. This happens when $n$ (the amount of features) is large, in which case Naive Bayes classifier would have to multiply a lot of numbers between 0 and 1 (remember that each $\\Pr(x_i \\mid y_k)$ is a probability value. This could cause some numerical problems, since multiplying a lot of numbers, all of them between 0 and 1, would result in a very tiny number, which may be not representable in somy systems. To circunvent that, most implementations of Naive Bayes do the following: instead of multiplying probabilites, they add log-probabilites. See below:\n",
        "\n",
        "$$\n",
        "\\Pr(x_1, x_2, \\dots x_n \\mid y_k) \\propto \\log(\\Pr(y_k)) + \\log(\\Pr(x_1 \\mid y_k)) + \\log(\\Pr(x_2 \\mid y_k)) + \\ldots + \\log(\\Pr(x_n \\mid y_k))\n",
        "$$\n",
        "\n",
        "The implementation trick presented above works because the class corresponding to the greatest value of $\\Pr(y_k) \\times \\Pi_{i=1}^{n} \\Pr(x_i \\mid y_k)$ is the same corresponding to the greatest value of $\\log(\\Pr(y_k)) + \\sum_{i=1}^{n} \\log(\\Pr(x_i \\mid y_k))$. You should notice that adding log-probabilities does not cause the same numerical problems found when multiplying probabilities.\n",
        "\n",
        "Therefore, to predict the class $\\hat{y}$ for the example represented by the features $x_1, \\dots, x_n$, the $\\text{argmax}$ operator is applied to the sum of the log-probabilites, as summarized below.\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} &= \\underset{k \\in \\{1, \\dots, |K|\\}}{\\text{argmax}} \\log \\left( \\Pr(y_k \\vert x_1, \\dots, x_n) \\right)\\\\\n",
        "&=  \\underset{k \\in \\{1, \\dots, |K|\\}}{\\text{argmax}} \\log \\left( \\Pr(y_k) \\displaystyle\\prod_{i=1}^n \\Pr(x_i \\vert y_k) \\right) \\\\\n",
        "&= \\underset{k \\in \\{1, \\dots, |K|\\}}{\\text{argmax}} \\left( \\log \\left( \\Pr(y_k) \\right) +  \\  \\displaystyle\\sum_{i=1}^n \\log \\left(\\Pr(x_i \\vert y_k) \\right) \\right)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL9GxHM6EqGO"
      },
      "source": [
        "# Dealing with continuous features\n",
        "\n",
        "The above description of the Naive Bayes algorithm was given in the context of discrete predictive features. However Naive Bayes can also be applied when the dataset contains continuous features. In this case, there are two possible alternatives.\n",
        "\n",
        "The first alternative is to discretize each continuous feature. One possibility for this is to use quartile ranges, such that values less than the 25th percentile are assigned a 1, 25th to 50th a 2, 50th to 75th a 3 and greater than the 75th percentile a 4. Thus, a single example will deposit one count in bin Q1, Q2, Q3, or Q4. After this discretization, the likelihood estimates can be computed on these categorical bins, as we saw earlier. Bin counts (probabilities) are then based on the number of samples whose variable values fall within a given bin.\n",
        "\n",
        "As a second alternative, the likelihood estimates are not computed by counting. Instead, the likelihood of each continuous predictive feature $x_i$ is computed considering that its values ​​follow a Gaussian distribution:\n",
        "\n",
        "$$\n",
        "\\Pr(x_i \\mid y = c_j) \\propto \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n",
        "$$\n",
        "\n",
        "In the above expression, $\\mu_y$ and $\\sigma_y$ are the mean and standard deviation of the values ​​of $x_j$, but only for examples related to the class $c_j$. Both $\\mu_y$ and $\\sigma_y$ are estimated from the training dataset.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1280px-Normal_Distribution_PDF.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWGi7m6vq02U"
      },
      "source": [
        "To give concrete examples of the first and second alternatives described above, consider again the Iris dataset. Remember that all the predictive features in the Iris dataset are continuous. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY7ybidvkohW",
        "outputId": "6494db3b-74c3-41fa-c8ac-aa99157a7572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "# if you'd like to check dataset type use: type(load_iris())\n",
        "# if you'd like to view list of attributes use: dir(load_iris())\n",
        "\n",
        "# np.c_ is the numpy concatenate function\n",
        "# which is used to concat iris['data'] and iris['target'] arrays \n",
        "# for pandas column argument: concat iris['feature_names'] list\n",
        "# and string list (in this case one string); you can make this anything you'd like..  \n",
        "# the original dataset would probably call this ['Species']\n",
        "df_iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
        "                     columns= iris['feature_names'] + ['target'])\n",
        "\n",
        "df_iris.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
              "0                5.1               3.5  ...               0.2     0.0\n",
              "1                4.9               3.0  ...               0.2     0.0\n",
              "2                4.7               3.2  ...               0.2     0.0\n",
              "3                4.6               3.1  ...               0.2     0.0\n",
              "4                5.0               3.6  ...               0.2     0.0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtYIkwGFdXjN"
      },
      "source": [
        "Concerning the first alternative, several discretization strategies are provided by Scikit-Learn class [KbinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer). See an example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIy1kFCEdMr_",
        "outputId": "39bd215e-756b-4348-d114-4df5178fb324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "# let us discretize the continuous features 'sepal length'\n",
        "sepal_length = df_iris['sepal length (cm)']\n",
        "\n",
        "discretizer = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\n",
        "\n",
        "df_iris['sepal length (cm) - discretized'] = discretizer.fit_transform(sepal_length.values.reshape(-1,1))\n",
        "\n",
        "# for visualization purposes, print the original and discretized features.\n",
        "df_iris[['sepal length (cm) - discretized', 'sepal length (cm)']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm) - discretized</th>\n",
              "      <th>sepal length (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>5.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>3.0</td>\n",
              "      <td>6.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>2.0</td>\n",
              "      <td>6.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>3.0</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>2.0</td>\n",
              "      <td>6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>2.0</td>\n",
              "      <td>5.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm) - discretized  sepal length (cm)\n",
              "0                                1.0                5.1\n",
              "1                                0.0                4.9\n",
              "2                                0.0                4.7\n",
              "3                                0.0                4.6\n",
              "4                                0.0                5.0\n",
              "..                               ...                ...\n",
              "145                              3.0                6.7\n",
              "146                              2.0                6.3\n",
              "147                              3.0                6.5\n",
              "148                              2.0                6.2\n",
              "149                              2.0                5.9\n",
              "\n",
              "[150 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwJoSn9ukfwu"
      },
      "source": [
        "For the second alternative, let us compute an estimate for the likelihood of a specific value of the continuous feature $\\text{sepal_length}$, that is, $\\Pr(\\text{sepal_length} = 5.1 \\mid \\text{species} = \\text{setosa})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n9p6LSKyn7g"
      },
      "source": [
        "Now, let us compute the mean and standard deviation for this features, but only considering the value $0.0$ for the target (which is the numeric code for setosa species)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_A-0YkKnaR4",
        "outputId": "12c602d2-a600-49e9-d6d3-bb5f49f90024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "df_iris_setosa = df_iris[df_iris.target == 0.0]\n",
        "\n",
        "mean_sepal_length = df_iris_setosa['sepal length (cm)'].mean()\n",
        "print(mean_sepal_length)\n",
        "\n",
        "std_sepal_length = df_iris_setosa['sepal length (cm)'].std()\n",
        "print(std_sepal_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.005999999999999\n",
            "0.3524896872134512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh9G5Ki0mqdX"
      },
      "source": [
        "We can finally compute the estimate for $\\Pr(\\text{sepal_length} = 5.1 \\mid \\text{species} = \\text{setosa})$. See the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJnOkiyWoKAc",
        "outputId": "4a774a08-1ca1-4267-ac36-992557a1c899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from scipy.stats import norm\n",
        "x  = 5.1\n",
        "norm.pdf(x, loc=mean_sepal_length, scale=std_sepal_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0922477665179735"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRzrTuiO3wL9",
        "outputId": "9549f1b4-aabd-4b97-f1a1-c60cb555ef88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from scipy.stats import norm\n",
        "x  = 4.6\n",
        "norm.pdf(x, loc=mean_sepal_length, scale=std_sepal_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5830198698153818"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLm9wydf93sS"
      },
      "source": [
        "# Naive Bayes in Scikit-Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqypBmvttKUO"
      },
      "source": [
        "## GaussianNB\n",
        "\n",
        "In Scikit-Learn, the class [GaussianNB](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes) implements the Gaussian Naive Bayes algorithm. This variant of the algorithm is useful when the predictive features are continuous numbers known to approximately follow the Gaussian distribution. \n",
        "\n",
        "The code cell below (adapted from [here](https://www.geeksforgeeks.org/naive-bayes-classifiers/)) shows an example of using the Gaussian Naive Bayes for classification on the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdO9vQ7xE0bt",
        "outputId": "9640c5b5-6e00-44ed-9316-762d89e8ded9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# load the iris dataset \n",
        "from sklearn.datasets import load_iris \n",
        "iris = load_iris() \n",
        "\n",
        "# store the feature matrix (X) and response vector (y) \n",
        "X = iris.data \n",
        "y = iris.target \n",
        "\n",
        "# splitting X and y into training and testing sets \n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
        "\n",
        "# training the model on training set \n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "gnb = GaussianNB() \n",
        "gnb.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFywsNm-NlZK"
      },
      "source": [
        "The `predict` method returns the inferred class for the provided examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlscsNs8Khpr",
        "outputId": "fedd5447-bb49-48bc-8467-651bf0f0bc0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# making predictions on the testing set \n",
        "y_pred = gnb.predict(X_test) \n",
        "\n",
        "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
        "from sklearn import metrics \n",
        "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lgvg9O409Cn"
      },
      "source": [
        "The `predict_proba` method produces the probability estimates for each example provided as argument. See the example below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGHRwOM_000e",
        "outputId": "c8e51159-8329-412f-b578-af09630bd2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gnb.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.00000000e+000, 4.85960219e-016, 2.24347925e-028],\n",
              "       [4.81032927e-029, 9.99999988e-001, 1.16657116e-008],\n",
              "       [8.55082235e-094, 9.69653014e-001, 3.03469859e-002],\n",
              "       [1.00000000e+000, 1.43672766e-014, 4.85229968e-027],\n",
              "       [1.81141042e-252, 4.01430066e-008, 9.99999960e-001],\n",
              "       [1.81704672e-113, 3.73062892e-001, 6.26937108e-001],\n",
              "       [3.00349394e-179, 6.53670270e-007, 9.99999346e-001],\n",
              "       [1.00000000e+000, 4.26226391e-011, 9.54068350e-022],\n",
              "       [1.00000000e+000, 4.42208196e-015, 7.70527673e-027],\n",
              "       [4.98375044e-215, 2.81534320e-008, 9.99999972e-001],\n",
              "       [5.82754537e-077, 9.99634071e-001, 3.65929013e-004],\n",
              "       [1.00000000e+000, 1.78045720e-012, 1.26705504e-023],\n",
              "       [3.19645958e-217, 5.66697917e-007, 9.99999433e-001],\n",
              "       [1.82392237e-100, 8.48782093e-001, 1.51217907e-001],\n",
              "       [6.88483197e-099, 9.55195818e-001, 4.48041816e-002],\n",
              "       [1.00000000e+000, 1.31188378e-016, 1.57589125e-029],\n",
              "       [1.72939933e-066, 9.99953970e-001, 4.60303620e-005],\n",
              "       [6.00311248e-097, 9.65219353e-001, 3.47806472e-002],\n",
              "       [1.00000000e+000, 2.56487548e-012, 3.73853769e-024],\n",
              "       [1.00000000e+000, 3.33019296e-016, 3.94536401e-029],\n",
              "       [5.69949366e-082, 9.99699116e-001, 3.00883644e-004],\n",
              "       [2.89510646e-096, 9.73284502e-001, 2.67154983e-002],\n",
              "       [3.44670432e-137, 3.60064094e-002, 9.63993591e-001],\n",
              "       [1.00000000e+000, 1.07144739e-015, 2.82710261e-028],\n",
              "       [1.83856418e-207, 2.28913424e-005, 9.99977109e-001],\n",
              "       [4.61419501e-073, 9.99657351e-001, 3.42649313e-004],\n",
              "       [1.00000000e+000, 2.21980438e-017, 2.30755160e-029],\n",
              "       [1.00000000e+000, 1.33859859e-015, 1.96204273e-027],\n",
              "       [4.47375288e-098, 9.92181642e-001, 7.81835798e-003],\n",
              "       [4.42105287e-145, 2.48728037e-002, 9.75127196e-001],\n",
              "       [6.03272053e-100, 9.56323001e-001, 4.36769985e-002],\n",
              "       [1.79267544e-249, 2.11779486e-009, 9.99999998e-001],\n",
              "       [7.33747663e-059, 9.99993649e-001, 6.35058362e-006],\n",
              "       [1.49702625e-192, 1.21254518e-005, 9.99987875e-001],\n",
              "       [4.36893203e-179, 9.96566789e-007, 9.99999003e-001],\n",
              "       [1.00000000e+000, 6.75030947e-016, 1.93782572e-027],\n",
              "       [3.29296702e-069, 9.99984843e-001, 1.51571083e-005],\n",
              "       [1.00000000e+000, 9.10365944e-016, 9.02238243e-028],\n",
              "       [1.39529648e-106, 8.93263555e-001, 1.06736445e-001],\n",
              "       [2.29089890e-190, 1.24216754e-003, 9.98757832e-001],\n",
              "       [3.65466238e-189, 5.79043639e-006, 9.99994210e-001],\n",
              "       [1.00000000e+000, 3.59222037e-015, 2.34019345e-027],\n",
              "       [1.45947185e-124, 9.72205301e-001, 2.77946988e-002],\n",
              "       [1.01669273e-174, 1.18289219e-003, 9.98817108e-001],\n",
              "       [1.68088179e-099, 9.72029753e-001, 2.79702465e-002],\n",
              "       [3.92919052e-309, 3.23293012e-010, 1.00000000e+000],\n",
              "       [1.00000000e+000, 1.99805458e-012, 2.98009258e-023],\n",
              "       [1.00000000e+000, 9.23545140e-017, 8.16523458e-029],\n",
              "       [1.00000000e+000, 1.89984128e-014, 3.50664019e-027],\n",
              "       [1.74777712e-067, 9.99909341e-001, 9.06586975e-005],\n",
              "       [1.00000000e+000, 1.51671980e-015, 2.83118999e-027],\n",
              "       [1.00000000e+000, 1.77722580e-015, 1.33863294e-027],\n",
              "       [5.12729719e-285, 4.78037292e-010, 1.00000000e+000],\n",
              "       [8.31346855e-184, 6.27973062e-006, 9.99993720e-001],\n",
              "       [1.14235929e-229, 1.13481293e-005, 9.99988652e-001],\n",
              "       [8.11621380e-199, 1.96110507e-006, 9.99998039e-001],\n",
              "       [2.12632207e-128, 9.61075946e-002, 9.03892405e-001],\n",
              "       [6.29573691e-102, 7.03622909e-001, 2.96377091e-001],\n",
              "       [1.21122614e-274, 7.60729859e-008, 9.99999924e-001],\n",
              "       [2.47437268e-073, 9.99476252e-001, 5.23747761e-004]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXUkMFYnJkfV"
      },
      "source": [
        "## MultinomialNB\n",
        "\n",
        "The [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) is the NB implementation to be used when the predictive features are discrete (as was the case in the Play Tenis dataset). From the Scikit-Learn documentation:\n",
        "\n",
        "> The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TosZiwppqFeB"
      },
      "source": [
        "## BernoulliNB\n",
        "\n",
        "The class [BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) implements the Naive Bayes algorithm for Bernoulli's multivariate models. A Bernoulli multivariate model considers that the predictive features are all binary (i.e., they assume values either 0 or 1). Hence Bernoulli Naive Bayes is a special case of Multinomial Naive Bayes."
      ]
    }
  ]
}